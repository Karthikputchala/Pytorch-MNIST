{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrX3h+li2ZGIukG+aWNlk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikputchala/Pytorch-MNIST/blob/main/Pytorch_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mEI_qjMUIP6w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the dataset\n",
        "def download_mnist_datasets():\n",
        "  # train data\n",
        "    train_data = datasets.MNIST(\n",
        "        root = \"data\",\n",
        "        download = True,\n",
        "        train = True,\n",
        "        transform = ToTensor()\n",
        "    )\n",
        "  # validation data\n",
        "    validation_data = datasets.MNIST(\n",
        "        root = \"data\",\n",
        "        download = True,\n",
        "        train = False,\n",
        "        transform = ToTensor()\n",
        "    )\n",
        "    return train_data, validation_data\n",
        "\n",
        "# creating a basic model\n",
        "def mnist():\n",
        "  model = nn.Sequential(\n",
        "     nn.Flatten(),\n",
        "     nn.Linear(28*28, 256),\n",
        "     nn.ReLU(),\n",
        "     nn.Linear(256, 10)\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# training an epoch\n",
        "def train_one_epoch(model, data_loader, loss_function, optimizer, device):\n",
        "  for inputs, targets in data_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # predictions\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # loss calculation\n",
        "    loss = loss_function(predictions, targets)\n",
        "\n",
        "    # backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"loss:{loss.item()}\")\n",
        "\n",
        "# training the model with data \n",
        "def train(model, data_loader, loss_function, optimizer, device, epochs):\n",
        "  for i in range(epochs):\n",
        "    print(\"epoch:{}\".format(i))\n",
        "    train_one_epoch(model, data_loader, loss_function, optimizer, device)\n",
        "    print(\"----------------------\")\n",
        "  print(\"Training is done\")\n",
        "\n",
        "# predicting the validation data\n",
        "def predict(model, inputs, targets, clas_mapping):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions = model(inputs)\n",
        "    predicted_index = predictions[0].argmax(0)\n",
        "    predicted = class_mapping[predicted_index]\n",
        "    expected = class_mapping[targets]\n",
        "    return predicted, expected\n",
        "\n",
        "# classes in the data\n",
        "class_mapping = [\n",
        "    \"0\",\n",
        "    \"1\",\n",
        "    \"2\",\n",
        "    \"3\",\n",
        "    \"4\",\n",
        "    \"5\",\n",
        "    \"6\",\n",
        "    \"7\",\n",
        "    \"8\",\n",
        "    \"9\"\n",
        "]"
      ],
      "metadata": {
        "id": "fi-YhFSSw9kg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the dataset\n",
        "train_data, validation_data = download_mnist_datasets()\n",
        "print(\"mnist dataset downloaded\")"
      ],
      "metadata": {
        "id": "v4-WN5l7bzB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "BATCH_SIZE = 128\n",
        "# data loaders\n",
        "train_data_loader  =  DataLoader(train_data, batch_size =  BATCH_SIZE)\n",
        "validation_data_loader  =  DataLoader(validation_data, batch_size =  BATCH_SIZE)"
      ],
      "metadata": {
        "id": "l_wGZifCN01l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print(device)\n",
        "\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mnist.parameters(), lr = learning_rate)\n",
        "mnist = mnist().to(device)\n",
        "\n",
        "# training the mnist model\n",
        "train(mnist, train_data_loader, loss_function, optimizer, device, epochs)"
      ],
      "metadata": {
        "id": "BTw0u6YpzgV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # evaluating for just one sample\n",
        "input, target = validation_data[0][0], validation_data[0][1]\n",
        "predicted, expected  = predict(mnist, input, target, class_mapping)\n",
        "print(\"predicted:{}, expected:{}\".format(predicted, expected))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ9Xb4r3TH6Z",
        "outputId": "b2d19726-a0c8-437c-92c2-19cb9be7d4e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted:7, expected:7\n"
          ]
        }
      ]
    }
  ]
}